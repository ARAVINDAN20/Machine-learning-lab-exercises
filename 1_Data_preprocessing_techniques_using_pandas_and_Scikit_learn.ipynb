{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: Data Preprocessing Techniques Using Pandas and Scikit-Learn\n",
        "\n",
        "## Aim\n",
        "To explore and apply various data preprocessing techniques using Pandas and Scikit-Learn for preparing datasets for machine learning tasks.\n",
        "\n",
        "## Objectives\n",
        "- Understand the importance of data preprocessing in machine learning.\n",
        "- Implement common preprocessing steps such as handling missing data, encoding categorical variables, scaling features, and splitting data.\n",
        "\n",
        "## Tools Used\n",
        "- **Pandas**: For data manipulation.\n",
        "- **Scikit-Learn**: For preprocessing utilities.\n",
        "- **NumPy**: For numerical operations.\n",
        "\n",
        "## Implementation\n",
        "\n",
        "### Step 1: Import Libraries\n",
        "```python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "```\n",
        "\n",
        "### Step 2: Load the Dataset\n",
        "```python\n",
        "# Sample dataset creation\n",
        "data = {\n",
        "    'Age': [25, 27, np.nan, 29, 24],\n",
        "    'Salary': [50000, 54000, np.nan, 62000, 58000],\n",
        "    'Gender': ['Male', 'Female', 'Female', 'Male', np.nan],\n",
        "    'Purchased': ['No', 'Yes', 'No', 'Yes', 'No']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the dataset\n",
        "print(\"Original Dataset:\")\n",
        "print(df)\n",
        "```\n",
        "\n",
        "### Step 3: Handle Missing Data\n",
        "```python\n",
        "# Handling missing numerical data with mean imputation\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df['Age'] = imputer.fit_transform(df[['Age']])\n",
        "df['Salary'] = imputer.fit_transform(df[['Salary']])\n",
        "\n",
        "# Handling missing categorical data with mode imputation\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "df['Gender'] = imputer.fit_transform(df[['Gender']])\n",
        "\n",
        "print(\"\\nDataset After Handling Missing Data:\")\n",
        "print(df)\n",
        "```\n",
        "\n",
        "### Step 4: Encode Categorical Variables\n",
        "```python\n",
        "# Label encoding for the target variable 'Purchased'\n",
        "label_encoder = LabelEncoder()\n",
        "df['Purchased'] = label_encoder.fit_transform(df['Purchased'])\n",
        "\n",
        "# One-hot encoding for the 'Gender' column\n",
        "one_hot = pd.get_dummies(df['Gender'], prefix='Gender', drop_first=True)\n",
        "df = pd.concat([df.drop('Gender', axis=1), one_hot], axis=1)\n",
        "\n",
        "print(\"\\nDataset After Encoding Categorical Variables:\")\n",
        "print(df)\n",
        "```\n",
        "\n",
        "### Step 5: Scale Features\n",
        "```python\n",
        "# Standard scaling for numerical features\n",
        "scaler = StandardScaler()\n",
        "df[['Age', 'Salary']] = scaler.fit_transform(df[['Age', 'Salary']])\n",
        "\n",
        "print(\"\\nDataset After Feature Scaling:\")\n",
        "print(df)\n",
        "```\n",
        "\n",
        "### Step 6: Split the Data into Training and Testing Sets\n",
        "```python\n",
        "# Split the dataset into features and target variable\n",
        "X = df.drop('Purchased', axis=1)\n",
        "y = df['Purchased']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"\\nTraining Features:\")\n",
        "print(X_train)\n",
        "print(\"\\nTesting Features:\")\n",
        "print(X_test)\n",
        "print(\"\\nTraining Target:\")\n",
        "print(y_train)\n",
        "print(\"\\nTesting Target:\")\n",
        "print(y_test)\n",
        "```\n",
        "\n",
        "### Step 7: Summary and Observations\n",
        "```python\n",
        "print(\"\\nSummary:\")\n",
        "print(\"1. Missing data was handled using mean and mode imputation.\")\n",
        "print(\"2. Categorical variables were encoded using label encoding and one-hot encoding.\")\n",
        "print(\"3. Numerical features were scaled using standard scaling.\")\n",
        "print(\"4. The dataset was split into training and testing sets for model evaluation.\")\n"
      ],
      "metadata": {
        "id": "dHJRUM-AIPdT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QdZrTLfIP9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}