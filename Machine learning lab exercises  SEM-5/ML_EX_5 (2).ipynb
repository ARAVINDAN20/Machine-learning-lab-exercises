{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "An-ASenaoqqR",
        "outputId": "a9de89d0-3be4-4053-e884-394733418235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - accuracy: 0.4409 - loss: 1.0928 - val_accuracy: 0.9091 - val_loss: 0.8778\n",
            "Epoch 2/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4194 - loss: 1.0485 - val_accuracy: 0.9091 - val_loss: 0.8540\n",
            "Epoch 3/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4477 - loss: 1.0633 - val_accuracy: 1.0000 - val_loss: 0.8305\n",
            "Epoch 4/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5731 - loss: 0.9939 - val_accuracy: 1.0000 - val_loss: 0.8068\n",
            "Epoch 5/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6127 - loss: 0.9889 - val_accuracy: 1.0000 - val_loss: 0.7850\n",
            "Epoch 6/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5942 - loss: 0.9690 - val_accuracy: 1.0000 - val_loss: 0.7627\n",
            "Epoch 7/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6890 - loss: 0.8973 - val_accuracy: 1.0000 - val_loss: 0.7396\n",
            "Epoch 8/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6822 - loss: 0.8840 - val_accuracy: 1.0000 - val_loss: 0.7176\n",
            "Epoch 9/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7624 - loss: 0.8246 - val_accuracy: 1.0000 - val_loss: 0.6954\n",
            "Epoch 10/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.7054 - loss: 0.8454 - val_accuracy: 1.0000 - val_loss: 0.6752\n",
            "Epoch 11/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7746 - loss: 0.7540 - val_accuracy: 1.0000 - val_loss: 0.6552\n",
            "Epoch 12/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7683 - loss: 0.7887 - val_accuracy: 1.0000 - val_loss: 0.6356\n",
            "Epoch 13/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8065 - loss: 0.7471 - val_accuracy: 1.0000 - val_loss: 0.6161\n",
            "Epoch 14/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8524 - loss: 0.6922 - val_accuracy: 1.0000 - val_loss: 0.5975\n",
            "Epoch 15/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8655 - loss: 0.6660 - val_accuracy: 1.0000 - val_loss: 0.5795\n",
            "Epoch 16/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8561 - loss: 0.6446 - val_accuracy: 1.0000 - val_loss: 0.5623\n",
            "Epoch 17/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8156 - loss: 0.6187 - val_accuracy: 1.0000 - val_loss: 0.5446\n",
            "Epoch 18/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8409 - loss: 0.6063 - val_accuracy: 1.0000 - val_loss: 0.5272\n",
            "Epoch 19/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8530 - loss: 0.5815 - val_accuracy: 1.0000 - val_loss: 0.5111\n",
            "Epoch 20/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8785 - loss: 0.5368 - val_accuracy: 1.0000 - val_loss: 0.4955\n",
            "Epoch 21/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8375 - loss: 0.5529 - val_accuracy: 1.0000 - val_loss: 0.4803\n",
            "Epoch 22/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8092 - loss: 0.5486 - val_accuracy: 1.0000 - val_loss: 0.4665\n",
            "Epoch 23/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8184 - loss: 0.5160 - val_accuracy: 1.0000 - val_loss: 0.4539\n",
            "Epoch 24/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8409 - loss: 0.4949 - val_accuracy: 1.0000 - val_loss: 0.4416\n",
            "Epoch 25/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8851 - loss: 0.4789 - val_accuracy: 1.0000 - val_loss: 0.4299\n",
            "Epoch 26/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8230 - loss: 0.4861 - val_accuracy: 1.0000 - val_loss: 0.4202\n",
            "Epoch 27/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8355 - loss: 0.4849 - val_accuracy: 1.0000 - val_loss: 0.4107\n",
            "Epoch 28/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.8910 - loss: 0.4431 - val_accuracy: 1.0000 - val_loss: 0.4007\n",
            "Epoch 29/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8871 - loss: 0.3992 - val_accuracy: 1.0000 - val_loss: 0.3910\n",
            "Epoch 30/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8467 - loss: 0.4286 - val_accuracy: 1.0000 - val_loss: 0.3833\n",
            "Epoch 31/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9091 - loss: 0.3777 - val_accuracy: 1.0000 - val_loss: 0.3746\n",
            "Epoch 32/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8062 - loss: 0.4769 - val_accuracy: 1.0000 - val_loss: 0.3665\n",
            "Epoch 33/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8474 - loss: 0.4288 - val_accuracy: 1.0000 - val_loss: 0.3586\n",
            "Epoch 34/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8395 - loss: 0.4000 - val_accuracy: 1.0000 - val_loss: 0.3501\n",
            "Epoch 35/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8968 - loss: 0.3496 - val_accuracy: 1.0000 - val_loss: 0.3427\n",
            "Epoch 36/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8985 - loss: 0.3847 - val_accuracy: 1.0000 - val_loss: 0.3359\n",
            "Epoch 37/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.8818 - loss: 0.3668 - val_accuracy: 1.0000 - val_loss: 0.3299\n",
            "Epoch 38/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9026 - loss: 0.3403 - val_accuracy: 1.0000 - val_loss: 0.3240\n",
            "Epoch 39/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8440 - loss: 0.3665 - val_accuracy: 1.0000 - val_loss: 0.3183\n",
            "Epoch 40/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8369 - loss: 0.3698 - val_accuracy: 1.0000 - val_loss: 0.3128\n",
            "Epoch 41/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.8832 - loss: 0.3451 - val_accuracy: 1.0000 - val_loss: 0.3082\n",
            "Epoch 42/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.8533 - loss: 0.3254 - val_accuracy: 1.0000 - val_loss: 0.3048\n",
            "Epoch 43/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - accuracy: 0.8807 - loss: 0.3027 - val_accuracy: 1.0000 - val_loss: 0.3014\n",
            "Epoch 44/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8822 - loss: 0.3288 - val_accuracy: 1.0000 - val_loss: 0.2970\n",
            "Epoch 45/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.9060 - loss: 0.2974 - val_accuracy: 1.0000 - val_loss: 0.2926\n",
            "Epoch 46/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.8721 - loss: 0.3438 - val_accuracy: 0.9091 - val_loss: 0.2902\n",
            "Epoch 47/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.9029 - loss: 0.2928 - val_accuracy: 0.9091 - val_loss: 0.2872\n",
            "Epoch 48/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.8799 - loss: 0.3082 - val_accuracy: 0.9091 - val_loss: 0.2836\n",
            "Epoch 49/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9191 - loss: 0.2768 - val_accuracy: 0.9091 - val_loss: 0.2782\n",
            "Epoch 50/50\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.9192 - loss: 0.2756 - val_accuracy: 0.9091 - val_loss: 0.2760\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        19\n",
            "           1       1.00      0.69      0.82        13\n",
            "           2       0.76      1.00      0.87        13\n",
            "\n",
            "    accuracy                           0.91        45\n",
            "   macro avg       0.92      0.90      0.89        45\n",
            "weighted avg       0.93      0.91      0.91        45\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 1.\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the MLP model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, input_dim=4, activation='relu'),  # hidden layer\n",
        "    tf.keras.layers.Dense(3, activation='softmax')  # output layer\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=10, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_classes))\n"
      ]
    },
    {
      "source": [
        "# 2.\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score # import the missing functions\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "\n",
        "X = data\n",
        "y = target\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the MLP model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(13, input_dim=13, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=10, validation_split=0.1)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqn4KYujppmC",
        "outputId": "c17d52b6-01a6-45f9-b80c-098292437d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 584.8505 - val_loss: 512.3097\n",
            "Epoch 2/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 506.5848 - val_loss: 486.8240\n",
            "Epoch 3/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 538.9301 - val_loss: 458.1954\n",
            "Epoch 4/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 444.6490 - val_loss: 427.0807\n",
            "Epoch 5/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 477.1913 - val_loss: 394.4424\n",
            "Epoch 6/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 365.5628 - val_loss: 361.2445\n",
            "Epoch 7/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 323.8911 - val_loss: 329.5197\n",
            "Epoch 8/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 322.2914 - val_loss: 298.4628\n",
            "Epoch 9/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 275.4883 - val_loss: 269.3605\n",
            "Epoch 10/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 250.8949 - val_loss: 243.7327\n",
            "Epoch 11/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 193.3417 - val_loss: 219.8633\n",
            "Epoch 12/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 201.0542 - val_loss: 198.4177\n",
            "Epoch 13/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 204.2601 - val_loss: 179.6372\n",
            "Epoch 14/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 162.6699 - val_loss: 162.9874\n",
            "Epoch 15/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 156.0226 - val_loss: 148.3650\n",
            "Epoch 16/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 123.8626 - val_loss: 134.4396\n",
            "Epoch 17/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 116.2337 - val_loss: 122.4507\n",
            "Epoch 18/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 100.3470 - val_loss: 112.0076\n",
            "Epoch 19/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 105.1481 - val_loss: 101.6826\n",
            "Epoch 20/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 102.5480 - val_loss: 92.4657\n",
            "Epoch 21/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 98.4818 - val_loss: 84.4940\n",
            "Epoch 22/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 73.5236 - val_loss: 77.3492\n",
            "Epoch 23/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65.8703 - val_loss: 71.1067\n",
            "Epoch 24/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.0511 - val_loss: 65.3960\n",
            "Epoch 25/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 51.8245 - val_loss: 60.7908\n",
            "Epoch 26/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56.2330 - val_loss: 56.9374\n",
            "Epoch 27/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 46.4654 - val_loss: 53.6195\n",
            "Epoch 28/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 49.7203 - val_loss: 50.4435\n",
            "Epoch 29/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 46.5375 - val_loss: 48.6944\n",
            "Epoch 30/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39.6348 - val_loss: 46.9966\n",
            "Epoch 31/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32.4577 - val_loss: 45.8565\n",
            "Epoch 32/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 44.5074 - val_loss: 44.7922\n",
            "Epoch 33/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 36.9451 - val_loss: 44.2906\n",
            "Epoch 34/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 30.2060 - val_loss: 43.7276\n",
            "Epoch 35/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 36.9336 - val_loss: 43.5617\n",
            "Epoch 36/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 33.6552 - val_loss: 43.1470\n",
            "Epoch 37/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 35.7483 - val_loss: 42.8026\n",
            "Epoch 38/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 30.5655 - val_loss: 42.6248\n",
            "Epoch 39/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 34.3834 - val_loss: 42.2852\n",
            "Epoch 40/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 28.8436 - val_loss: 42.3279\n",
            "Epoch 41/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 24.1758 - val_loss: 42.2065\n",
            "Epoch 42/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.5705 - val_loss: 42.0386\n",
            "Epoch 43/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 27.9825 - val_loss: 41.6689\n",
            "Epoch 44/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 25.1868 - val_loss: 41.4785\n",
            "Epoch 45/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 27.7791 - val_loss: 41.1877\n",
            "Epoch 46/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 28.0888 - val_loss: 41.1873\n",
            "Epoch 47/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 35.8068 - val_loss: 40.9781\n",
            "Epoch 48/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 27.5674 - val_loss: 40.6902\n",
            "Epoch 49/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.5840 - val_loss: 40.9425\n",
            "Epoch 50/50\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 23.6784 - val_loss: 40.8200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fc4c71c45e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/5\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 52ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 11 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7fc4c71c45e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step \n",
            "Mean Squared Error: 22.221574074220637\n",
            "R-squared Score: 0.7017762595303657\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# Import the MLPRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# 3. Reinitialize and train the MLP regressor with different hyperparameters\n",
        "mlp_optimized = MLPRegressor(hidden_layer_sizes=(20, 20), max_iter=2000, learning_rate_init=0.01, random_state=42)\n",
        "mlp_optimized.fit(X_train, y_train)\n",
        "\n",
        "# Predict and analyze performance\n",
        "y_pred_optimized = mlp_optimized.predict(X_test)\n",
        "\n",
        "# Use appropriate metrics for regression\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred_optimized))\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred_optimized))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W2njKdkWq8Y_",
        "outputId": "9617b277-e795-44a0-dca3-add21761609a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 12.59321291413075\n",
            "R-squared Score: 0.8309932929486097\n"
          ]
        }
      ]
    },
    {
      "source": [
        "#4. Import necessary libraries\n",
        "from sklearn.svm import SVR # Changed SVC to SVR for regression\n",
        "\n",
        "# Initialize and train the SVM regressor\n",
        "svm = SVR(kernel='linear') # Changed SVC to SVR for regression\n",
        "svm.fit(X_train, y_train)\n",
        "\n",
        "# Predict and analyze performance\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "\n",
        "# Use appropriate metrics for regression\n",
        "from sklearn.metrics import mean_squared_error, r2_score # import metrics for regression\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred_svm))\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred_svm))"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFNT-pL2rKYQ",
        "outputId": "eb16cc0a-b272-44ef-8054-ffe7023f8696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 24.971388169372414\n",
            "R-squared Score: 0.6648724901433158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Import necessary libraries\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Initialize and train the SVM regressor\n",
        "svm_reg = SVR(kernel='linear')\n",
        "svm_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and analyze performance\n",
        "y_pred_svm_reg = svm_reg.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred_svm_reg))\n",
        "print(\"R^2 Score:\", r2_score(y_test, y_pred_svm_reg))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68iNLmAxpxGj",
        "outputId": "00485b07-25fd-41f7-a961-148c93036723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 24.971388169372414\n",
            "R^2 Score: 0.6648724901433158\n"
          ]
        }
      ]
    },
    {
      "source": [
        "# 6.\n",
        "# Reinitialize and train the SVM regressor with different hyperparameters - Use SVR for regression\n",
        "svm_optimized = SVR(kernel='rbf', C=1, gamma=0.1) # Change model to SVR\n",
        "svm_optimized.fit(X_train, y_train)\n",
        "\n",
        "# Predict and analyze performance\n",
        "y_pred_svm_optimized = svm_optimized.predict(X_test)\n",
        "# Evaluate the model - Use metrics suitable for regression\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred_svm_optimized))\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred_svm_optimized)) # Change metrics to suit regression"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKmCNw-7qjDp",
        "outputId": "92747c0f-8ccc-4ea1-e54f-e459cd2970cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 27.38779858377615\n",
            "R-squared Score: 0.6324431514346205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go through the code from your Python file step by step, explaining each part and relating it to the specific tasks you're asked to accomplish.\n",
        "\n",
        "### Task 1: Classification with MLP on the Iris Dataset\n",
        "\n",
        "#### **Code Explanation:**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "```\n",
        "- **Libraries Imported:**\n",
        "  - `tensorflow`: For building and training neural networks.\n",
        "  - `load_iris`: Loads the Iris dataset, a standard dataset in machine learning.\n",
        "  - `train_test_split`: Splits the dataset into training and testing sets.\n",
        "  - `StandardScaler`: Standardizes features by removing the mean and scaling to unit variance.\n",
        "  - `classification_report`: Provides a detailed report on classification performance.\n",
        "\n",
        "```python\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "```\n",
        "- **Loading Data:**\n",
        "  - `X` contains the feature data (sepal length, sepal width, petal length, petal width).\n",
        "  - `y` contains the target labels (Iris species).\n",
        "\n",
        "```python\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "```\n",
        "- **Data Splitting:** The dataset is divided into 70% training data and 30% testing data.\n",
        "\n",
        "```python\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "- **Standardization:** Ensures that the data has a mean of 0 and a standard deviation of 1, which helps the model converge more effectively.\n",
        "\n",
        "```python\n",
        "# Build the MLP model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, input_dim=4, activation='relu'),  # hidden layer\n",
        "    tf.keras.layers.Dense(3, activation='softmax')  # output layer\n",
        "])\n",
        "```\n",
        "- **Model Architecture:**\n",
        "  - The model is a Sequential neural network with:\n",
        "    - A hidden layer of 10 neurons with ReLU activation.\n",
        "    - An output layer with 3 neurons (one for each class) using softmax activation for multi-class classification.\n",
        "\n",
        "```python\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "```\n",
        "- **Compiling the Model:**\n",
        "  - `optimizer='adam'`: The Adam optimizer is used for its efficiency in training.\n",
        "  - `loss='sparse_categorical_crossentropy'`: Used for multi-class classification problems.\n",
        "  - `metrics=['accuracy']`: The model's performance will be evaluated based on accuracy.\n",
        "\n",
        "```python\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=10, validation_split=0.1)\n",
        "```\n",
        "- **Training the Model:**\n",
        "  - The model is trained for 50 epochs with a batch size of 10.\n",
        "  - `validation_split=0.1` means that 10% of the training data is used for validation during training.\n",
        "\n",
        "```python\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = tf.argmax(y_pred, axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_test, y_pred_classes))\n",
        "```\n",
        "- **Evaluation:**\n",
        "  - `y_pred_classes` contains the predicted class labels.\n",
        "  - The classification report provides precision, recall, F1-score, and support for each class.\n",
        "\n",
        "### Task 2: Regression with MLP on the Boston Housing Dataset\n",
        "\n",
        "#### **Code Explanation:**\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "target = raw_df.values[1::2, 2]\n",
        "```\n",
        "- **Loading Boston Housing Data:**\n",
        "  - The dataset is loaded from an external URL, with specific preprocessing steps to extract the feature matrix `data` and the target variable `target`.\n",
        "\n",
        "```python\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "```\n",
        "- **Data Splitting and Scaling:** The dataset is split into training and test sets, and feature scaling is applied similarly to the classification task.\n",
        "\n",
        "```python\n",
        "# Build the MLP model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(13, input_dim=13, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "```\n",
        "- **MLP Model for Regression:**\n",
        "  - The model has one hidden layer with 13 neurons.\n",
        "  - The output layer has a single neuron, suitable for regression tasks.\n",
        "  - The loss function is `mean_squared_error`, appropriate for regression.\n",
        "\n",
        "```python\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=10, validation_split=0.1)\n",
        "```\n",
        "- **Training:** Similar to the classification model, but now focused on predicting continuous values.\n",
        "\n",
        "```python\n",
        "# Evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred))\n",
        "```\n",
        "- **Evaluation:**\n",
        "  - `mean_squared_error`: Measures the average squared difference between actual and predicted values.\n",
        "  - `r2_score`: Indicates how well the model explains the variance in the target variable.\n",
        "\n",
        "### Task 3: Improving MLP Performance by Hyperparameter Tuning\n",
        "\n",
        "```python\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "# Reinitialize and train the MLP regressor with different hyperparameters\n",
        "mlp_optimized = MLPRegressor(hidden_layer_sizes=(20, 20), max_iter=2000, learning_rate_init=0.01, random_state=42)\n",
        "mlp_optimized.fit(X_train, y_train)\n",
        "```\n",
        "- **MLP Regressor with Optimized Hyperparameters:**\n",
        "  - `hidden_layer_sizes=(20, 20)`: Two hidden layers with 20 neurons each.\n",
        "  - `max_iter=2000`: Allows the model more iterations to converge.\n",
        "  - `learning_rate_init=0.01`: A lower learning rate for more precise weight updates.\n",
        "\n",
        "```python\n",
        "# Predict and analyze performance\n",
        "y_pred_optimized = mlp_optimized.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred_optimized))\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred_optimized))\n",
        "```\n",
        "- **Evaluation:** Performance is evaluated using the same metrics, comparing with the initial model.\n",
        "\n",
        "### Task 4: Regression with SVM\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "# Initialize and train the SVM regressor\n",
        "svm = SVR(kernel='linear')\n",
        "svm.fit(X_train, y_train)\n",
        "```\n",
        "- **Support Vector Regressor:**\n",
        "  - `SVR`: A regression-based model using Support Vector Machines.\n",
        "  - `kernel='linear'`: A linear kernel is used.\n",
        "\n",
        "```python\n",
        "# Predict and analyze performance\n",
        "y_pred_svm = svm.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred_svm))\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred_svm))\n",
        "```\n",
        "- **Evaluation:** The performance of the SVM regressor is evaluated similarly to the MLP models.\n",
        "\n",
        "### Task 5: Optimized SVM Regression\n",
        "\n",
        "```python\n",
        "# Reinitialize and train the SVM regressor with different hyperparameters\n",
        "svm_optimized = SVR(kernel='rbf', C=1, gamma=0.1)\n",
        "svm_optimized.fit(X_train, y_train)\n",
        "```\n",
        "- **SVM with RBF Kernel:**\n",
        "  - `kernel='rbf'`: Radial Basis Function kernel, better suited for non-linear data.\n",
        "  - `C=1`, `gamma=0.1`: Hyperparameters for regularization and kernel coefficient, respectively.\n",
        "\n",
        "```python\n",
        "# Predict and analyze performance\n",
        "y_pred_svm_optimized = svm_optimized.predict(X_test)\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred_svm_optimized))\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred_svm_optimized))\n",
        "```\n",
        "- **Evaluation:** As before, but now assessing the performance of the SVM with optimized parameters.\n",
        "\n",
        "---\n",
        "\n",
        "### Summary of Key Concepts:\n",
        "- **MLP for Classification and Regression**: Different structures for different tasks.\n",
        "- **Hyperparameter Tuning**: Adjusting parameters like layer size, learning rate, and kernel choice to improve model performance.\n",
        "- **Evaluation Metrics**: `classification_report`, `mean_squared_error`, and `r2_score` are crucial in understanding model performance.\n",
        "\n",
        "This code provides a comprehensive understanding of how to approach classification and regression tasks using both MLP and SVM, with an emphasis on model tuning and evaluation."
      ],
      "metadata": {
        "id": "RWLWjs_ctzDL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6: Optimizing SVM Regression with Different Hyperparameters\n",
        "\n",
        "#### **Objective:**\n",
        "\n",
        "The goal of this task is to improve the performance of the Support Vector Regression (SVR) model by reinitializing it with different hyperparameters, specifically using a non-linear kernel (RBF) and adjusting other hyperparameters like `C` and `gamma`. The model is then evaluated using appropriate regression metrics.\n",
        "\n",
        "#### **Code Explanation:**\n",
        "\n",
        "```python\n",
        "# Reinitialize and train the SVM regressor with different hyperparameters - Use SVR for regression\n",
        "svm_optimized = SVR(kernel='rbf', C=1, gamma=0.1) # Change model to SVR\n",
        "svm_optimized.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "- **Support Vector Regression (SVR):**\n",
        "  - **`SVR`**: SVR is a regression technique that extends the concept of Support Vector Machines (SVM) from classification to regression. It aims to find a function that deviates from the actual target values by a margin that's less than or equal to a threshold (`epsilon`), while simultaneously being as flat as possible.\n",
        "\n",
        "- **Hyperparameters:**\n",
        "  - **`kernel='rbf'`**: The Radial Basis Function (RBF) kernel is a popular choice for non-linear data. It maps the input data into a higher-dimensional space where it is more likely to be linearly separable (or easier to fit a regression function).\n",
        "  - **`C=1`**: The regularization parameter `C` controls the trade-off between achieving a low error on the training data and minimizing the norm of the weights. A smaller `C` value makes the decision surface smoother, while a larger `C` aims to fit the training data as closely as possible.\n",
        "  - **`gamma=0.1`**: The `gamma` parameter defines how far the influence of a single training example reaches, with low values indicating 'far' and high values indicating 'close'. This essentially controls the shape of the decision boundary.\n",
        "\n",
        "- **Model Fitting:**\n",
        "  - **`svm_optimized.fit(X_train, y_train)`**: The SVR model is trained on the training data with the specified hyperparameters. The model learns to map the input features (`X_train`) to the target values (`y_train`).\n",
        "\n",
        "```python\n",
        "# Predict and analyze performance\n",
        "y_pred_svm_optimized = svm_optimized.predict(X_test)\n",
        "```\n",
        "\n",
        "- **Prediction:**\n",
        "  - **`y_pred_svm_optimized`**: The trained SVR model is used to make predictions on the test set (`X_test`). The predicted values are stored in `y_pred_svm_optimized`.\n",
        "\n",
        "```python\n",
        "# Evaluate the model - Use metrics suitable for regression\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred_svm_optimized))\n",
        "print(\"R-squared Score:\", r2_score(y_test, y_pred_svm_optimized)) # Change metrics to suit regression\n",
        "```\n",
        "\n",
        "- **Evaluation:**\n",
        "  - **`mean_squared_error`**: This metric measures the average squared difference between the actual target values (`y_test`) and the predicted values (`y_pred_svm_optimized`). It quantifies how much the predicted values deviate from the actual values. Lower values indicate better model performance.\n",
        "  - **`r2_score`**: The R-squared score is a statistical measure that represents the proportion of the variance for the dependent variable (target) that's explained by the independent variables (features) in the model. An R-squared score closer to 1 indicates that the model explains a large portion of the variance in the target variable.\n",
        "\n",
        "#### **Summary of Task 6:**\n",
        "\n",
        "In this task, you optimized the performance of the SVR model by choosing a non-linear kernel (RBF) and fine-tuning the hyperparameters `C` and `gamma`. This was done to better capture the underlying patterns in the data, especially if the relationship between features and target is non-linear. The model's performance was evaluated using `Mean Squared Error` and `R-squared Score`, which are standard metrics for regression tasks. The changes in hyperparameters and the choice of a non-linear kernel aimed to improve the accuracy and predictive power of the SVR model."
      ],
      "metadata": {
        "id": "-hR5lSYDuKLc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rc-VQPJruLKA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}