{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 6: Backpropagation Algorithm for Training MLP\n",
        "\n",
        "# Aim\n",
        "To implement the backpropagation algorithm for training a Multilayer Perceptron (MLP) using Scikit-learn and TensorFlow, visualize the dataset, and print all outputs step-by-step.\n",
        "\n",
        "# Algorithm\n",
        "1. **Define the Problem**:\n",
        "   - Create or load a dataset.\n",
        "   - Define the target and features.\n",
        "\n",
        "2. **Create the Dataset**:\n",
        "   - Use synthetic data generation libraries like `numpy` or `sklearn`.\n",
        "\n",
        "3. **Preprocess the Data**:\n",
        "   - Normalize the data.\n",
        "   - Split into training and test sets.\n",
        "\n",
        "4. **Build the Model**:\n",
        "   - Use Scikit-learn's `MLPClassifier`.\n",
        "   - Implement a TensorFlow-based model with a custom backpropagation process.\n",
        "\n",
        "5. **Train the Model**:\n",
        "   - Train using the backpropagation algorithm.\n",
        "   - Use gradient descent to update weights.\n",
        "\n",
        "6. **Visualize Training Process**:\n",
        "   - Plot the training and validation loss over epochs.\n",
        "\n",
        "7. **Evaluate the Model**:\n",
        "   - Calculate metrics like accuracy and confusion matrix.\n",
        "\n",
        "8. **Visualize Results**:\n",
        "   - Plot decision boundaries and confusion matrix.\n",
        "\n",
        "9. **Document Observations**:\n",
        "   - Comment on results and improvements.\n",
        "\n",
        "---\n",
        "\n",
        "# Implementation\n",
        "```python\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# 1. Create Dataset\n",
        "np.random.seed(42)\n",
        "num_samples = 500\n",
        "X1 = np.random.normal(0, 1, (num_samples, 2)) + 2\n",
        "X2 = np.random.normal(0, 1, (num_samples, 2)) - 2\n",
        "X = np.vstack((X1, X2))\n",
        "y = np.hstack((np.zeros(num_samples), np.ones(num_samples)))\n",
        "\n",
        "# Visualize the dataset\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
        "plt.title(\"Synthetic Dataset\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n",
        "\n",
        "# 2. Preprocess Data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Build and Train Scikit-learn MLPClassifier\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(10, 5), activation='relu', solver='adam', max_iter=500, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "\n",
        "# 4. Evaluate Scikit-learn Model\n",
        "sklearn_train_acc = accuracy_score(y_train, mlp.predict(X_train))\n",
        "sklearn_test_acc = accuracy_score(y_test, mlp.predict(X_test))\n",
        "print(f\"Scikit-learn MLP Train Accuracy: {sklearn_train_acc * 100:.2f}%\")\n",
        "print(f\"Scikit-learn MLP Test Accuracy: {sklearn_test_acc * 100:.2f}%\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, mlp.predict(X_test))\n",
        "ConfusionMatrixDisplay(cm).plot(cmap='viridis')\n",
        "plt.title(\"Confusion Matrix (Scikit-learn)\")\n",
        "plt.show()\n",
        "\n",
        "# 5. Build and Train TensorFlow Model\n",
        "model = Sequential([\n",
        "    Dense(10, activation='relu', input_shape=(2,)),\n",
        "    Dense(5, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, verbose=0)\n",
        "\n",
        "# 6. Evaluate TensorFlow Model\n",
        "loss, tf_test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"TensorFlow MLP Test Accuracy: {tf_test_acc * 100:.2f}%\")\n",
        "\n",
        "# Plot Training and Validation Loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# 7. Decision Boundary Visualization\n",
        "xx, yy = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n",
        "grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "preds = model.predict(grid).reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, preds, levels=[0, 0.5, 1], cmap='viridis', alpha=0.7)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', edgecolor='k')\n",
        "plt.title(\"Decision Boundary (TensorFlow)\")\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.show()\n",
        "\n",
        "# Observations\n",
        "print(\"Both Scikit-learn and TensorFlow models achieved high accuracy, demonstrating the effectiveness of the backpropagation algorithm in training MLPs. TensorFlow's visualization of training dynamics is particularly helpful.\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lkW16WHiKCyX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CpaKbBzFKDSa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}